{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22b3dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved with 81 words.\n",
      "Model saved to caption_model.keras\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Add\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Generate synthetic captions (for demonstration purposes)\n",
    "subjects = [\n",
    "    \"a man\", \"a woman\", \"a child\", \"a boy\", \"a girl\", \"a dog\", \"a cat\", \"a bird\", \"a robot\", \"a cyclist\",\n",
    "    \"an artist\", \"a teacher\", \"a chef\", \"a student\", \"a scientist\", \"a musician\", \"a photographer\", \n",
    "    \"a farmer\", \"a soldier\", \"an astronaut\"\n",
    "]\n",
    "\n",
    "verbs = [\n",
    "    \"is running\", \"is jumping\", \"is playing\", \"is driving\", \"is eating\", \"is sleeping\", \"is flying\", \n",
    "    \"is painting\", \"is singing\", \"is teaching\", \"is walking\", \"is exploring\", \"is climbing\", \n",
    "    \"is working\", \"is building\", \"is designing\", \"is studying\", \"is observing\", \"is shopping\", \"is relaxing\"\n",
    "]\n",
    "\n",
    "objects = [\n",
    "    \"in the park\", \"on the road\", \"at home\", \"in the sky\", \"in the kitchen\", \"on a mountain\", \"on a beach\", \n",
    "    \"in the forest\", \"by the river\", \"in a school\", \"in a city\", \"on a boat\", \"in a car\", \"on a plane\", \n",
    "    \"in the desert\", \"on a rooftop\", \"in a garden\", \"under the stars\", \"on a bridge\", \"at a market\"\n",
    "]\n",
    "\n",
    "adjectives = [\"happy\", \"angry\", \"excited\", \"tired\", \"curious\"]\n",
    "adverbs = [\"quickly\", \"gracefully\", \"loudly\", \"eagerly\", \"slowly\"]\n",
    "\n",
    "captions = []\n",
    "for _ in range(10000):  # Generate 10000 synthetic captions\n",
    "    subject = random.choice(subjects)\n",
    "    verb = random.choice(verbs)\n",
    "    object_ = random.choice(objects)\n",
    "    adjective = random.choice(adjectives)\n",
    "    adverb = random.choice(adverbs)\n",
    "\n",
    "    caption = f\"startseq {adjective} {subject} {verb} {adverb} {object_} endseq\"\n",
    "    captions.append(caption)\n",
    "\n",
    "# Step 2: Initialize the tokenizer and fit on the captions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "\n",
    "# Save the tokenizer\n",
    "with open(\"tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "print(f\"Tokenizer saved with {len(tokenizer.word_index)} words.\")\n",
    "\n",
    "# Step 3: Define parameters for the captioning model\n",
    "max_sequence_length = 30\n",
    "embedding_dim = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "image_feature_size = 2048  # Size of the features from InceptionV3\n",
    "\n",
    "# Load InceptionV3 model for image feature extraction\n",
    "inception_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "inception_model = Model(inputs=inception_model.input, outputs=inception_model.output)\n",
    "\n",
    "def extract_features(image_path):\n",
    "    \"\"\"Extract features from an image using InceptionV3.\"\"\"\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "    features = inception_model.predict(image, verbose=0)\n",
    "    return features\n",
    "\n",
    "# Step 4: Define the captioning model\n",
    "image_input = tf.keras.Input(shape=(image_feature_size,))\n",
    "image_features = Dense(embedding_dim, activation='relu')(image_input)\n",
    "\n",
    "caption_input = tf.keras.Input(shape=(max_sequence_length,))\n",
    "caption_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(caption_input)\n",
    "caption_lstm = LSTM(256)(caption_embedding)\n",
    "\n",
    "decoder_input = Add()([image_features, caption_lstm])\n",
    "output = Dense(vocab_size, activation='softmax')(decoder_input)\n",
    "\n",
    "caption_model = Model(inputs=[image_input, caption_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Save the trained model in the Keras format\n",
    "caption_model.save(\"caption_model.keras\")\n",
    "print(\"Model saved to caption_model.keras\")\n",
    "\n",
    "# Step 5: Example of how to train the model\n",
    "# When you have image features and tokenized captions, you can train the model:\n",
    "# X_image_features = np.array([...])  # Extracted features from images\n",
    "# X_captions = pad_sequences([...], maxlen=max_sequence_length)  # Tokenized captions\n",
    "# y_output = np.array([...])  # One-hot encoded labels for next word prediction\n",
    "\n",
    "# Train the model (example code, not functional without actual data):\n",
    "# caption_model.fit([X_image_features, X_captions], y_output, epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa8c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025429136520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025429136520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: startseq relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing relaxing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "# Load the trained model\n",
    "caption_model = tf.keras.models.load_model(\"caption_model.keras\")\n",
    "\n",
    "# Define parameters\n",
    "max_sequence_length = 30  # Maximum length of captions\n",
    "embedding_dim = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "image_feature_size = 2048  # Size of the features from InceptionV3\n",
    "\n",
    "# Load InceptionV3 model for image feature extraction\n",
    "inception_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "inception_model = Model(inputs=inception_model.input, outputs=inception_model.output)\n",
    "\n",
    "# Apply Global Average Pooling to reduce the shape of the output to (1, 2048)\n",
    "def extract_features(image_path):\n",
    "    \"\"\"Extract features from an image using InceptionV3.\"\"\"\n",
    "    image = load_img(image_path, target_size=(299, 299))  # Resize image to 299x299\n",
    "    image = img_to_array(image)  # Convert to array\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = tf.keras.applications.inception_v3.preprocess_input(image)  # Preprocess image for InceptionV3\n",
    "    \n",
    "    # Extract features from InceptionV3\n",
    "    features = inception_model.predict(image, verbose=0)\n",
    "    \n",
    "    # Apply Global Average Pooling to reduce the feature map shape\n",
    "    pooling_layer = GlobalAveragePooling2D()\n",
    "    features = pooling_layer(features)\n",
    "    \n",
    "    # Now the features have shape (1, 2048)\n",
    "    return features\n",
    "\n",
    "def generate_caption(image_path, model, tokenizer, max_sequence_length):\n",
    "    \"\"\"Generate a caption for a given image.\"\"\"\n",
    "    # Extract image features\n",
    "    image_features = extract_features(image_path)\n",
    "    \n",
    "    # Start the caption with the 'startseq' token\n",
    "    caption = 'startseq'\n",
    "    \n",
    "    for _ in range(max_sequence_length):\n",
    "        # Tokenize the caption\n",
    "        sequence = tokenizer.texts_to_sequences([caption])\n",
    "        sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict([image_features, sequence], verbose=0)\n",
    "        \n",
    "        # Get the word index of the predicted word\n",
    "        predicted_word_index = np.argmax(predicted_probs)\n",
    "        \n",
    "        # Get the word corresponding to the predicted index\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        \n",
    "        # If the predicted word is 'endseq', stop generating the caption\n",
    "        if predicted_word == 'endseq':\n",
    "            break\n",
    "        \n",
    "        # Append the predicted word to the caption\n",
    "        caption += ' ' + predicted_word\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Example: Predict a caption for a sample image\n",
    "image_path = 'dog1.jpeg'  # Replace with your image path\n",
    "generated_caption = generate_caption(image_path, caption_model, tokenizer, max_sequence_length)\n",
    "\n",
    "# Print the generated caption\n",
    "print(\"Generated Caption:\", generated_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b69f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
